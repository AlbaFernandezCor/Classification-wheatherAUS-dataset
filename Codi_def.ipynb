{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÀCTICA 2 - CLASSIFICACIÓ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1597487 | Manuel Arnau Fernández\n",
    "\n",
    "1600123 | Alba Fernández Coronado\n",
    "\n",
    "1605547 | Marina Palomar González"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregant les llibreries necessàries\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import collections as col\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats\n",
    "import random as rand\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, scale, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor, RANSACRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error , r2_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import warnings\n",
    "import time\n",
    "from yellowbrick.target import FeatureCorrelation\n",
    "import statistics as st\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. CLASSIFICACIÓ NUMÈRICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obrint el dataset\n",
    "dataset = pd.read_csv('./weatherAUS.csv')\n",
    "print(f'Mida de les dades: {dataset.shape}')\n",
    "dataset.info()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target: variable objectiu\n",
    "Y = dataset['RainTomorrow']\n",
    "\n",
    "print(\"Tipus de valors: \",Y.nunique()) #quants tipus de valors hi ha: 2\n",
    "print(\"Valors únics: \",Y.unique()) #quins són els valors únics: 0, 1\n",
    "print(Y.value_counts()/len(Y)*100) #percentatge de cada resposta\n",
    "print(\"% NaNs: \",Y.isnull().sum()/len(Y)*100) #percentatge NaNs\n",
    "sns.catplot(x = 'RainTomorrow', data = dataset, kind = \"count\", palette='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudi de la correlació entre els atributs\n",
    "correlacio = dataset.corr()\n",
    "plt.figure(figsize=(12,10))\n",
    "ax = sns.heatmap(correlacio, annot=True, linewidths=.5, cmap = sns.diverging_palette(20, 220, n=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,15))\n",
    "ax = fig.gca()\n",
    "dataset.hist(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_replace(df):\n",
    "    import datetime as dt\n",
    "    month = []\n",
    "    for num in df['Date']:\n",
    "        date_obj = dt.datetime.strptime(num,\"%Y-%m-%d\")\n",
    "        date_mon = date_obj.month\n",
    "        month.append(date_mon)\n",
    "    season_options = ['Summer', 'Summer', 'Autumn', 'Autumn', 'Autumn', 'Winter', 'Winter', 'Winter', 'Spring', 'Spring', 'Spring','Summer']\n",
    "    seasons= []\n",
    "    for i in month:\n",
    "        seasons.append(season_options[i-1])\n",
    "    n = df.columns[0]\n",
    "    df.drop(n, axis = 1, inplace = True)\n",
    "    df['Seasons'] = seasons\n",
    "    df = df[['Seasons'] +  [col for col in df.columns if col != 'Seasons']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.copy()\n",
    "dataseason = season_replace(data)\n",
    "\n",
    "rainfall =[dataseason['Seasons'], dataseason['Rainfall']]\n",
    "headers = ['Seasons', 'Rainfall']\n",
    "rain_df = pd.concat(rainfall, axis=1, keys=headers)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "a = rain_df.groupby('Seasons').agg({'Rainfall':'sum'})\n",
    "a.plot(kind='bar', color='pink')\n",
    "plt.title('Rainfall distribution in each season')\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('Rainfall (in mm)')\n",
    "plt.xticks(rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "current_cmap = matplotlib.cm.get_cmap()\n",
    "current_cmap.set_bad(color='red')\n",
    "plt.imshow(dataset.isna(),aspect = 'auto')\n",
    "plt.show()\n",
    "# aquestes dues columnes mb tant de nans es poden eliminar pq no ens produeixen informació, segurament, \n",
    "# les seves mesures estan preses per una estació meteorologica està espatllada\n",
    "\n",
    "# plotting the number of rows with entries per column\n",
    "msno.bar(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Fem una copia del dataset\n",
    "dataset_cp = dataset.copy()\n",
    "\n",
    "#Eliminem les files que continguin NaNs a la variable relacionada amb puja\n",
    "# dataset_cp = dataset_cp[dataset_cp['RainTomorrow'].notna()]\n",
    "# dataset_cp = dataset_cp[dataset_cp['RainToday'].notna()]\n",
    "# dataset_cp = dataset_cp[dataset_cp['Rainfall'].notna()]\n",
    "dataset_cp = dataset_cp.dropna(axis=0,how='any',subset=[\"RainTomorrow\", \"Rainfall\", \"RainToday\"])\n",
    "\n",
    "# Mirem el percentatge de NanNs de cada variable\n",
    "print((dataset_cp.isnull().sum()/len(dataset_cp))*100)\n",
    "\n",
    "#eliminem les columnes amb percentatges alts de NaNs\n",
    "dataset_cp = dataset_cp.drop(['Evaporation'], axis=1)\n",
    "dataset_cp = dataset_cp.drop(['Sunshine'], axis=1)\n",
    "dataset_cp = dataset_cp.drop(['Cloud9am'], axis=1)\n",
    "dataset_cp = dataset_cp.drop(['Cloud3pm'], axis=1)\n",
    "\n",
    "#omplim les columnes amb un percentatge mes baix amb la moda dels valors no nulls de la columna\n",
    "dataset_cp['MinTemp'].fillna(dataset_cp['MinTemp'].mode()[0], inplace=True)\n",
    "dataset_cp['MaxTemp'].fillna(dataset_cp['MaxTemp'].mode()[0], inplace=True)\n",
    "dataset_cp['Rainfall'].fillna(dataset_cp['Rainfall'].mode()[0], inplace=True)\n",
    "dataset_cp['WindGustDir'].fillna(dataset_cp['WindGustDir'].mode()[0], inplace=True)\n",
    "dataset_cp['WindGustSpeed'].fillna(dataset_cp['WindGustSpeed'].mode()[0], inplace=True)\n",
    "dataset_cp['WindDir9am'].fillna(dataset_cp['WindDir9am'].mode()[0], inplace=True)\n",
    "dataset_cp['WindDir3pm'].fillna(dataset_cp['WindDir3pm'].mode()[0], inplace=True)\n",
    "dataset_cp['WindSpeed9am'].fillna(dataset_cp['WindSpeed9am'].mode()[0], inplace=True)\n",
    "dataset_cp['WindSpeed3pm'].fillna(dataset_cp['WindSpeed3pm'].mode()[0], inplace=True)\n",
    "dataset_cp['Humidity9am'].fillna(dataset_cp['Humidity9am'].mode()[0], inplace=True)\n",
    "dataset_cp['Humidity3pm'].fillna(dataset_cp['Humidity3pm'].mode()[0], inplace=True) \n",
    "dataset_cp['Pressure9am'].fillna(dataset_cp['Pressure9am'].mode()[0], inplace=True)\n",
    "dataset_cp['Pressure3pm'].fillna(dataset_cp['Pressure3pm'].mode()[0], inplace=True)\n",
    "dataset_cp['Temp9am'].fillna(dataset_cp['Temp9am'].mode()[0], inplace=True)\n",
    "dataset_cp['Temp3pm'].fillna(dataset_cp['Temp3pm'].mode()[0], inplace=True)\n",
    "dataset_cp['RainToday'].fillna(dataset_cp['RainToday'].mode()[0], inplace=True)\n",
    "\n",
    "dataset_cp['Year'] = dataset_cp['Date'].str.split(\"-\", n=1).str[0].astype(\"int64\")\n",
    "dataset_cp['Month'] = dataset_cp['Date'].str.split(\"-\", n=2).str[1].astype(\"int64\")\n",
    "dataset_cp['Day'] = dataset_cp['Date'].str.split(\"-\", n=3).str[2].astype(\"int64\")\n",
    "dataset_cp = dataset_cp.drop(['Date'], axis=1)\n",
    "\n",
    "# Es transformes totes les dades a numeriques\n",
    "dataset_cp = pd.concat([dataset_cp, pd.get_dummies(dataset_cp.RainToday, drop_first=True, prefix='RainToday')], axis=1)\n",
    "dataset_cp = dataset_cp.drop(['RainToday'], axis=1)\n",
    "dataset_cp = pd.concat([dataset_cp, pd.get_dummies(dataset_cp.RainTomorrow, drop_first=True, prefix='RainTomorrow')], axis=1)\n",
    "dataset_cp = dataset_cp.drop(['RainTomorrow'], axis=1)\n",
    "le = LabelEncoder()\n",
    "dataset_cp['Location'] = le.fit_transform(dataset_cp['Location'])\n",
    "dataset_cp['WindGustDir'] = le.fit_transform(dataset_cp['WindGustDir'])\n",
    "dataset_cp['WindDir9am'] = le.fit_transform(dataset_cp['WindDir9am'])\n",
    "dataset_cp['WindDir3pm'] = le.fit_transform(dataset_cp['WindDir3pm'])\n",
    "\n",
    "dataset_cp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S'agafen tots els atributs que no siguin object\n",
    "NoObj_data = dataset[dataset.select_dtypes(exclude=['object']).columns]\n",
    "\n",
    "# Agafem la intersecció entre el dataset_cp i el dataset sense atributs objecte per tractar els outliers posteriorment\n",
    "intersect = list(set(dataset_cp.columns).intersection(list(NoObj_data.columns)))\n",
    "NoObj_data = dataset_cp[intersect]\n",
    "\n",
    "NoObj_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(dataset_cp['Rainfall'], color= 'green', edgecolor = 'black', alpha = 0.6)\n",
    "plt.xlabel('Rainfall')\n",
    "plt.show()\n",
    "sns.distplot(dataset_cp['Rainfall'], color= 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treiem la columna Rainfall perque els valors extrems no es consideren outliers\n",
    "NoObj_data = NoObj_data.drop(['Rainfall'], axis = 1)\n",
    "\n",
    "MiceImputed = NoObj_data.copy(deep=True) \n",
    "mice_imputer = IterativeImputer()\n",
    "MiceImputed.iloc[:, :] = mice_imputer.fit_transform(NoObj_data)\n",
    "\n",
    "# Detecting outliers with IQR\n",
    "Q1 = MiceImputed.quantile(0.05)\n",
    "Q3 = MiceImputed.quantile(0.95)\n",
    "IQR = Q3 - Q1\n",
    "total_outlier_num = ((NoObj_data < (Q1 - 1.5 * IQR)) | (NoObj_data > (Q3 + 1.5 * IQR))).sum()\n",
    "print(total_outlier_num)\n",
    "\n",
    "# Removing outliers from the dataset\n",
    "dataset_cp = dataset_cp[~((MiceImputed < (Q1 - 1.5 * IQR)) |(MiceImputed > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "dataset_cp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlacio dels atributs amb la variable objectiu\n",
    "X = dataset_cp.drop(['RainTomorrow_Yes'], axis=1)\n",
    "Y = dataset_cp['RainTomorrow_Yes']\n",
    "feature_names = list(X.columns)\n",
    "visualizer = FeatureCorrelation(labels = feature_names)\n",
    "visualizer.fit(X, Y)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr = dataset_cp.corr()\n",
    "corr1 = pd.DataFrame(abs(corr['RainTomorrow_Yes']),columns = ['RainTomorrow_Yes'])\n",
    "nonvals = corr1.loc[corr1['RainTomorrow_Yes'] < 0.05]\n",
    "print('Var correlation < 5%',nonvals)\n",
    "nonvals = list(nonvals.index.values)\n",
    "\n",
    "# We extract variables with correlation less than 5%\n",
    "dataset_cp = dataset_cp.drop(columns=nonvals,axis=1)\n",
    "print('Data Final',dataset_cp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Es separen X i Y del dataset\n",
    "X = dataset_cp.drop(['RainTomorrow_Yes'], axis=1)\n",
    "Y = dataset_cp['RainTomorrow_Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalitzem les dades\n",
    "scaler = StandardScaler()\n",
    "X_scale = pd.DataFrame(scaler.fit_transform(X))\n",
    "X_cols = X.columns\n",
    "X_scale.columns = X_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#es fa un pca per visualitzar els outliers\n",
    "model = PCA(n_components=8)\n",
    "model.fit(X_scale)\n",
    "data_pca = pd.DataFrame(\n",
    "    data = model.components_,\n",
    "    columns = X.columns,\n",
    "    index = ['PC1','PC2','PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8'])\n",
    "X_pca = model.transform(X_scale)\n",
    "\n",
    "# Percentatge de variança de cada component\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "ax.bar(\n",
    "    x      = np.arange(model.n_components_) + 1,\n",
    "    height = model.explained_variance_ratio_\n",
    ")\n",
    "\n",
    "for x, y in zip(np.arange(len(data_pca.columns)) + 1, model.explained_variance_ratio_):\n",
    "    label = round(y, 2)\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        (x,y),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0,10),\n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "ax.set_xticks(np.arange(model.n_components_) + 1)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_title('Percentatge de variança explicat per cada ')\n",
    "ax.set_xlabel('Component principal')\n",
    "ax.set_ylabel('% variança explicat');\n",
    "\n",
    "#Conclusió: com les variances són molt baixes, no val la pena fer un PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No val la pena fer un polynomial features perque tenim masses característiques i trigariem molt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separem en train i test\n",
    "x_train,x_test,y_train,y_test = train_test_split(X_scale,Y,test_size=0.3, random_state = 0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE SELECTION\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def BWSelection_VIF(Xtrain):\n",
    "    # VIF dataframe\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = Xtrain.columns\n",
    "    # calculating VIF for each feature\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(Xtrain.values, i)\n",
    "                              for i in range(len(Xtrain.columns))]\n",
    "\n",
    "    #si el valor de VIF és molt alt, hi ha molta correlacio entre variables\n",
    "    if max(vif_data[\"VIF\"]) > 5:\n",
    "        X_new = Xtrain.drop(vif_data['feature'][np.argmax(vif_data[\"VIF\"])],axis=1)\n",
    "        BWSelection_VIF(X_new)\n",
    "    else:\n",
    "        llistaVIF.append([col for col in Xtrain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llistaVIF =[]\n",
    "BWSelection_VIF(X_scale)\n",
    "llistaVIF = llistaVIF[0]\n",
    "llistaVIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MÚLTIPLE DECISION TREE CLASSIFIER\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#Treballem en paral·lel per fer un model simple per a cada atribut considerat important en el VIF_Selection\n",
    "#A partir d'aquí decidirem amb quins models treballar\n",
    "def MultipleDT(atribut):\n",
    "    \n",
    "    clf_dt = DecisionTreeClassifier(random_state=0)\n",
    "    clf_dt.fit(x_train[[atribut]],y_train)\n",
    "    y_pred1 = clf_dt.predict(x_test[[atribut]])\n",
    "    score1 = accuracy_score(y_test,y_pred1)\n",
    "    y_pred2 = clf_dt.predict(x_train[[atribut]])\n",
    "    score2 = accuracy_score(y_train,y_pred2)\n",
    "    return [score1,score2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time.time() \n",
    "result = Parallel(n_jobs = 10)(delayed(MultipleDT)(atribut) for atribut in llistaVIF)\n",
    "\n",
    "for atribut,r in zip(llistaVIF,result): \n",
    "        print('Accuracy',atribut,':',r) #r = [acc_test, acc_train]\n",
    "print('Time taken :' , time.time()-t0, \"s\")\n",
    "\n",
    "print(\"Null accuracy: \",y_test.value_counts()[0]/sum(y_test.value_counts())) #Percentatge de 0 al y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# SVM\n",
    "def MultipleSVC(k):\n",
    "    model = svm.SVC(kernel = k) #classificació\n",
    "    model.fit(x_train[['Humidity3pm']], y_train)\n",
    "    y_pred = model.predict(x_test[['Humidity3pm']])\n",
    "    score = accuracy_score(y_test,y_pred)\n",
    "    return score\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "def LogReg():\n",
    "    logreg = LogisticRegression(solver='saga', random_state=0)\n",
    "    logreg.fit(x_train[llistaVIF], y_train)\n",
    "    score_test = logreg.score(x_test[llistaVIF], y_test)\n",
    "    score_train = logreg.score(x_train[llistaVIF], y_train)\n",
    "    LM_cm = confusion_matrix(y_test, logreg.predict(x_test[llistaVIF]))\n",
    "    return logreg, score_test, score_train, LM_cm\n",
    "    \n",
    "# KNN\n",
    "def KNN(n=5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn.fit(x_train[llistaVIF],y_train)\n",
    "    score_test = knn.score(x_test[llistaVIF], y_test)\n",
    "    score_train = knn.score(x_train[llistaVIF], y_train)\n",
    "    KNN_cm = confusion_matrix(y_test, knn.predict(x_test[llistaVIF]))\n",
    "    return knn, score_test, score_train, KNN_cm\n",
    "    \n",
    "#Preceptron model\n",
    "def Perc():\n",
    "    clf = Perceptron(random_state=0)\n",
    "    clf.fit(x_train, y_train)\n",
    "    score_test = clf.score(x_test,y_test)\n",
    "    score_train = clf.score(x_train,y_train)\n",
    "    return clf, score_test, score_train\n",
    "\n",
    "#Random Forest\n",
    "def RandomForest():\n",
    "    modelrfcla = RandomForestClassifier(n_estimators=50,random_state=9,n_jobs=-1).fit(x_train[llistaVIF],y_train)\n",
    "    score_test = modelrfcla.score(x_test[llistaVIF], y_test)\n",
    "    score_train = modelrfcla.score(x_train[llistaVIF], y_train)\n",
    "    RF_cm = confusion_matrix(y_test, modelrfcla.predict(x_test[llistaVIF]))\n",
    "    return modelrfcla, score_test, score_train, RF_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "print(\"MODEL SVM\")\n",
    "t0=time.time()        \n",
    "kernels = [\"sigmoid\", \"rbf\"]\n",
    "result = Parallel(n_jobs = 2)(delayed(MultipleSVC)(kernel) for kernel in kernels)\n",
    "for kernel,r in zip(kernels,result):\n",
    "        print('Accuracy',kernel,':',r)\n",
    "print('Time taken :' , time.time()-t0)\n",
    "\n",
    "# LR\n",
    "print(\"MODEL LOGISTIC REGRESSION\")\n",
    "t0=time.time()  \n",
    "LR, log_acctest, log_acctrain, LR_cm = LogReg()\n",
    "print(\"Accuracy test:\",log_acctest)\n",
    "print(\"Accuracy train:\",log_acctrain)\n",
    "print('Time taken :' , time.time()-t0)\n",
    "\n",
    "# KNN\n",
    "print(\"MODEL KNN\")\n",
    "t0=time.time()\n",
    "KNN, knn_acctest, knn_acctrain, KNN_cm = KNN()\n",
    "print(\"Accuracy test:\",knn_acctest)\n",
    "print(\"Accuracy train:\",knn_acctrain)\n",
    "print('Time taken :' , time.time()-t0)\n",
    "\n",
    "#Perceptron\n",
    "print(\"MODEL PERCEPTRON\")\n",
    "t0=time.time() \n",
    "Perc, perc_acctest, perc_acctrain = Perc()\n",
    "print(\"Accuracy test:\",perc_acctest)\n",
    "print(\"Accuracy train:\",perc_acctrain)\n",
    "print('Time taken :' , time.time()-t0)\n",
    "\n",
    "#Random Forest\n",
    "print(\"MODEL RANDOM FOREST\")\n",
    "t0=time.time() \n",
    "RF, RF_acctest, RF_acctrain, RF_cm = RandomForest()\n",
    "print(\"Accuracy test:\",RF_acctest)\n",
    "print(\"Accuracy train:\",RF_acctrain)\n",
    "print('Time taken :' , time.time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSSVALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ens qdem amb el logistic regression, Random forest i amb el KNN q son els millors models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def CV (k):\n",
    "    skf = StratifiedKFold(n_splits=k)\n",
    "    skf.get_n_splits(X, Y)\n",
    "    StratifiedKFold(n_splits=k, random_state=None, shuffle=True)\n",
    "    i = 1\n",
    "    for train_index, test_index in skf.split(X_scale, Y):\n",
    "        X_train, X_test = X_scale.iloc[train_index], X_scale.iloc[test_index]\n",
    "        Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "        #Logistic Regression\n",
    "        LR.fit(X_train[llistaVIF], Y_train)\n",
    "        y_pred = LR.predict(X_test[llistaVIF])\n",
    "        score = accuracy_score(Y_test, y_pred)\n",
    "        accLM.append(score)\n",
    "#         print(\"Accuracy Logistic Model: \",score,\"fold: \", i)\n",
    "\n",
    "        #KNN\n",
    "        KNN.fit(X_train[llistaVIF], Y_train)\n",
    "        pred_test = KNN.predict(X_test[llistaVIF])\n",
    "        score = accuracy_score(Y_test, pred_test)\n",
    "        accKNN.append(score)\n",
    "#         print(\"Accuracy KNN Model: \",score,\"fold: \", i)\n",
    "\n",
    "        #RandomForest\n",
    "        RF.fit(X_train[llistaVIF], Y_train)\n",
    "        pred_test = RF.predict(X_test[llistaVIF])\n",
    "        score = accuracy_score(Y_test, pred_test)\n",
    "        accRF.append(score)\n",
    "#         print(\"Accuracy RandomForest Model: \",score,\"fold: \", i)\n",
    "        i = i+1\n",
    "    return accLM, accKNN, accRF\n",
    "\n",
    "#No  seria convenient aplicar LeaveOneOut, ja que en aquest dataset hi ha  moltes files i trigariem molt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(range(2,8))\n",
    "for split in k:\n",
    "    print(\"CrossValidation amb k = \",split)\n",
    "    accLM = []\n",
    "    accKNN = []\n",
    "    accRF = []\n",
    "    accLM, accKNN, accRF = CV(split)\n",
    "    plt.plot(list(range(2,split+1)), accLM, label = \"LR - Accuracy mitjà: {:.3f}\".format(st.mean(accLM)))\n",
    "    plt.plot(list(range(2,split+1)), accKNN, label = \"KNN - Accuracy mitjà: {:.3f}\".format(st.mean(accKNN)))\n",
    "    plt.plot(list(range(2,split+1)), accRF, label = \"RF - Accuracy mitjà: {:.3f}\".format(st.mean(accRF)))\n",
    "    plt.legend(loc = \"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METRIC ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = fig.add_subplot(1, 3, 1) \n",
    "ax1.set_title('Logistic Regression Classification') \n",
    "ax2 = fig.add_subplot(1, 3, 2) \n",
    "ax2.set_title('KNN Classification')\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "ax3.set_title('Random Forest Classification')\n",
    "sns.heatmap(data=LR_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax1)\n",
    "sns.heatmap(data=KNN_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax2)  \n",
    "sns.heatmap(data=RF_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "#Logistic Regression\n",
    "visualizerLR = ROCAUC(LR)\n",
    "\n",
    "visualizerLR.fit(x_train[llistaVIF], y_train)        # Fit the training data to the visualizer\n",
    "visualizerLR.score(x_test[llistaVIF], y_test)        # Evaluate the model on the test data\n",
    "visualizerLR.show()                       # Finalize and show the figure\n",
    "\n",
    "#KNN\n",
    "KNN.fit(x_train[llistaVIF], y_train)\n",
    "visualizerKNN = ROCAUC(KNN)\n",
    "\n",
    "visualizerKNN.fit(x_train[llistaVIF], y_train)        # Fit the training data to the visualizer\n",
    "visualizerKNN.score(x_test[llistaVIF], y_test)        # Evaluate the model on the test data\n",
    "visualizerKNN.show()                       # Finalize and show the figure\n",
    "\n",
    "\n",
    "#RandomForest\n",
    "RF.fit(x_train[llistaVIF], y_train)\n",
    "visualizerRF = ROCAUC(RF)\n",
    "\n",
    "visualizerRF.fit(x_train[llistaVIF], y_train)        # Fit the training data to the visualizer\n",
    "visualizerRF.score(x_test[llistaVIF], y_test)        # Evaluate the model on the test data\n",
    "visualizerRF.show()                       # Finalize and show the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score, average_precision_score\n",
    "# Compute Precision-Recall and plot curve LogisticRegression\n",
    "def PrecisionRecall (model):\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    average_precision = {}\n",
    "    plt.figure()\n",
    "    probs = model.predict_proba(x_test[llistaVIF])\n",
    "\n",
    "    for i in range(2):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test == i, probs[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test == i, probs[:, i])\n",
    "        plt.plot(recall[i], precision[i],\n",
    "        label='Precision-recall curve of class {0} (area = {1:0.2f})'\n",
    "                               ''.format(i, average_precision[i]))\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        \n",
    "    plt.title(model)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [LR, KNN, RF]\n",
    "for m in models:\n",
    "    PrecisionRecall(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#Logistic Regression\n",
    "print(\"LOGISTIC REGRESSION CLASSIFICATION REPORT\")\n",
    "print(classification_report(y_test, LR.predict(x_test[llistaVIF])))\n",
    "\n",
    "#KNN\n",
    "print(\"KNN CLASSIFICATION REPORT\")\n",
    "print(classification_report(y_test, KNN.predict(x_test[llistaVIF])))\n",
    "\n",
    "#Random Forest\n",
    "print(\"RANDOM FOREST CLASSIFICATION REPORT\")\n",
    "print(classification_report(y_test, RF.predict(x_test[llistaVIF])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPERPARAMETER SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Logistic Regression\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear','saga']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = StratifiedKFold(n_splits=10, random_state=1, shuffle = True)\n",
    "grid_search = GridSearchCV(estimator=LR, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_scale[llistaVIF],Y)\n",
    "# summarize results\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "n_neighbors = [1,2,5,10]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(n_neighbors = n_neighbors)\n",
    "cv = StratifiedKFold(n_splits=10, random_state=1, shuffle = True)\n",
    "grid_search = GridSearchCV(estimator=KNN, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_scale[llistaVIF],Y)\n",
    "# summarize results\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF\n",
    "n_estimators = [10,25,50]\n",
    "max_features = ['sqrt']\n",
    "min_samples_split = [1000,500,50,10]\n",
    "max_depth= [10,25,None] \n",
    "# define grid search\n",
    "grid = dict(n_estimators=n_estimators,max_features=max_features,min_samples_split=min_samples_split, max_depth = max_depth)\n",
    "cv = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "grid_search = GridSearchCV(estimator=RF, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_scale, Y)\n",
    "# summarize results\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. COMPARATIVA DE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "\n",
    "# Take the first two features.\n",
    "X = X_scale[['Humidity3pm','Rainfall']]\n",
    "n_classes = 2\n",
    "fig, sub = plt.subplots(1, 2, figsize=(16,6))\n",
    "sub[0].scatter(X[['Humidity3pm']], Y, c=Y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "sub[1].scatter(X[['Rainfall']], Y, c=Y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
    "\n",
    "particions = [0.5]\n",
    "\n",
    "for part in particions:\n",
    "    x_t, x_v, y_t, y_v = train_test_split(X, Y, train_size=part)\n",
    "    \n",
    "    #Creem el regresor logístic\n",
    "    logireg = LogisticRegression(C=2.0, fit_intercept=True, penalty='l2', tol=0.001)\n",
    "\n",
    "    # l'entrenem\n",
    "    logireg.fit(x_t, y_t)\n",
    "\n",
    "    print (\"Correct classification Logistic \", part, \"% of the data: \", logireg.score(x_v, y_v))\n",
    "    \n",
    "    #Creem el super vector machine\n",
    "    svc = svm.SVC(C=10.0, kernel='rbf', gamma=0.9, probability=True,max_iter=10000)\n",
    "\n",
    "    # l'entrenem \n",
    "    svc.fit(x_t, y_t)\n",
    "    probs = svc.predict_proba(x_v)\n",
    "    print (\"Correct classification SVM      \", part, \"% of the data: \", svc.score(x_v, y_v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "def show_C_effect(C=1.0, gamma=0.7, degree=3):\n",
    "\n",
    "    # import some data to play with\n",
    "    # Take the first two features. We could avoid this by using a two-dim dataset\n",
    "    X = dataset_cp[['Humidity3pm','Rainfall']]\n",
    "\n",
    "    # we create an instance of SVM and fit out data. We do not scale our\n",
    "    # data since we want to plot the support vectors\n",
    "    # title for the plots\n",
    "    titles = (\n",
    "              'LinearSVC C=0.1','LinearSVC C=1','LinearSVC C=10','LinearSVC C=100'\n",
    "              \n",
    "              )\n",
    "\n",
    "    models = (svm.LinearSVC(C=0.1,max_iter=10000),\n",
    "              svm.LinearSVC(C=1, max_iter=10000),\n",
    "              svm.LinearSVC(C=10, max_iter=10000),\n",
    "              svm.LinearSVC(C=100, max_iter=10000))\n",
    "                \n",
    "            \n",
    "    models = (clf.fit(X, Y) for clf in models)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig, sub = plt.subplots(2, 2, figsize=(14,9))\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    X0 = np.linspace(np.min(X[['Humidity3pm']]),np.max(X[['Humidity3pm']]),100)\n",
    "    X1 = np.linspace(np.min(X[['Rainfall']]),np.max(X[['Rainfall']]),100)\n",
    "\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "    for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "        plot_contours(ax, clf, xx, yy,\n",
    "                      cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        ax.scatter(X['Humidity3pm'],X['Rainfall'],c=Y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xlabel('Humidity3pm')\n",
    "        ax.set_ylabel('Rainfall')\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_C_effect(C=0.1) # c molt gran, farà underfitting (posarà tots els coeficients a zero), train sempre ho farà pitjor, \n",
    "                    # allí on el train i test es trobin, c és la optima\n",
    "                    # fer grafica C-J on J=-sum(ylog(1-yp)+(1-y)log(yp)+c*sum(h^2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
